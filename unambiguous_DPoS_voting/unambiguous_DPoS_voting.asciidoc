= Unambiguous Delegated Proof of Stake (DPoS) voting

== Background

The Delegated Proof of Stake (DPoS) consensus system requires that those who produce blocks, in the blockchain, assume their position by way of a fair vote. The blockchain community, who hold tokens, are by and large the ones tasked with casting these votes.

There are many terminologies for those who produce blocks. For example in EOS, those who produce blocks are called Block Producers (BPs). To become a Block Producer in EOS, one must first become a Block Producer Candidate (BPC). BPCs are required to publicly respond to a variety of criteria, and in doing so may fill the community with confidence; ultimately securing enough votes to become one of the many block producers.

In CyberMiles those who produce blocks are called Validators. As with EOS block producers, CyberMiles Validators are tasked with running crucial CyberMiles blockchain network infrastructure. Again similarly, all CyberMiles Validator Candidates are required to rally the community and in doing so, may succeed in becoming one of the many CyberMiles Validators.

The EOS network has 21 Block Producers

The CyberMiles network has 19 Validators

== Voting

It can be argued that voting in this arena is synonymous with prioritisation. For example, in order to successfully vote, a blockchain community member must evaluate all of the candidates while considering their qualitative responses to the vast criteria. It can also be argued that due to the sheer pace, noise and complexity in this space, voting can become onerous and potentially subjective as well as ambiguous.

So how does one calmly and confidently evaluate block producer candidates in order to vote? How does one equaluate the qualitative responses to criteria and produce a quantitative outcome which can inform their vote.

=== Basic example

This is a simple example of how *not* to evaluate crietia and candidates.

The following table shows the outcome of simple scoring. In this table there are 4 criteria; technical ability, community engagement, qualifications and future planning (scaling). These are just very simple examples to make a point. There are much more comprehensive examples (such as https://github.com/CyberMiles/tim-research/blob/master/eosio/eosio_dawn_3_0_and_dawn_4_0_research_report.asciidoc#block-producer-check-mark-criteria[the actual current EOS criteria]).

In this table there are also 2 candidates; candidate A and candidate B.

image:images/table_1.png[]

Legend::
1:: Poor
2:: Below average
3:: Average
4:: Above average
5:: Excellent

If we use the legend above, the sum (total) of the individual scores for each of the candidates will be 13. 

image:images/a_b_1.png[]

If we calculate the average score of each candidate we will end up with 3.25. In both of these cases it seems that these candidates are equally in the running for the user's vote.

image:images/a_b_2.png[]

If we look closely at the criteria we will see that candidate A and B are similar, except for community engagement and future planning (scaling). Whilst trying to quantify the criteria in this example, we actually just ended up with an even tie. It seems that there is a problem with this method of evaluation and in order to make a decision, using this system, the user needs to now revert back to qualitative and subjective decision making. 

Is it fair to say that it would be a bad decision to elect a candidate with "poor" future planning (scaling) provisions? Conversely, is it fair to say that it is a bad decision to elect a candidate with "below average" community engagement? 

In this simple example we can see how simple scoring with linear/sequential numbering is not ideal. It seems we need a better quantitative system.

=== Proposed solutions

==== Weighting

We could provide a weighting to each of the criteria. For example technical ability could be worth 30%, community engagement could be worth 20%, qualifications could be worth 20% and future planning (scaling) could be worth 30%. A total of 100%. 

image:images/table_3.png[]

When employing weighting, we see that candidate A has a total percentage of 60% and candidate B has a total percentage of 68%. As we can see, given this scenario which weighs technical ability and future planning (scaling) at a higher percentage (30% each) than community engagement and qualifications (20% each) we have a clear winner.

image:images/a_b_3.png[]

*Unfortunately, there are two problems with this approach.*

Firstly, if new criteria is added as time goes by (as was the case with the recent EOS candidature criteria) all of the weighting has to be recalculated. Prior evaluations have to be recalculated.

Secondly, there is no clear way to determine who is responsible for creating the weighting percentages. What if each community member has a slightly different view on what is important and what should carry the most/least weight? If one source, such as the blockchain software developers made a static decision on the weighting, this may be considered a form of centralization; that's decisions being made that may be in favour of colleagues or third-party companies etc.

==== Geometric Mean

Another approach is to employ the geometric mean. We saw above that using the arithmetic mean resulted in an even tie (even though the responses to the criteria were quite different). We calculate the arithmetic mean by adding all of the values up and then dividing that total by the number of criteria. For example (3 + 5 + 4 + 1) / 4 = 3.25.

The geometric mean is a little different. We calculate the geometric mean like this. 4âˆš 3 * 5 * 4 * 1 = 2.78.

If we apply this to both sets of data we see that candidate A has a geometric mean of 2.78 and candidate B has a geometric mean of 3.08. 

image:images/a_b_4.png[]

Using the geometric mean provides us with somewhat of a risk averse evaluation. This is because it provides an evennes which is, in a way, related to the fundamentals of ratios. Take the following example. If we are working with the numbers 9 and 4 we would have an arithmetic mean of 6.5 and a geometric mean of 6. Here is where it gets interesting. If we list our numbers with the geometric mean in the center we get 4, 6, 9. The first two numbers (as a fraction) 4/6 can be reduced to 2/3. Interestingly, the last two numbers (as a fraction) 6/9 can also be reduced to 2/3. In the event that the variables which contribute to the scoring of the evaluation are diverse the geometric provides a way to equalize skewed data for decision making.

Without any weighting being put in place this method organically gave candidate B the higher score, leaving candidate A (with the risky outlier of 1 - "poor" for future planning (scaling) in second place.








